install.packages('mice')
install.packages("VIM")
#combinations within the dataset.
install.packages('mice')
library(mice) #Load the multivariate imputation by chained equations library.
dim(sleep)
sum(is.na(sleep))
?md.pattern
mice::md.pattern(sleep) #Can also view this information from a data perspective.
help(sleep) #Inspecting the mammal sleep dataset.
sleep
summary(sleep) #Summary information for the sleep dataset.
sapply(sleep, sd) #Standard deviations for the sleep dataset; any issues?
VIM::aggr(sleep) #A graphical interpretation of the missing values and their
##################################
#####Visualizing Missing Data#####
##################################
install.packages('VIM')
install.packages("VIM")
library(VIM) #For the visualization and imputation of missing values.
help(sleep) #Inspecting the mammal sleep dataset.
sleep
summary(sleep) #Summary information for the sleep dataset.
sapply(sleep, sd) #Standard deviations for the sleep dataset; any issues?
VIM::aggr(sleep) #A graphical interpretation of the missing values and their
#combinations within the dataset. red means missing value
install.packages('mice')
library(mice) #Load the multivariate imputation by chained equations library.
dim(sleep)
sum(is.na(sleep))
?md.pattern
mice::md.pattern(sleep) #Can also view this information from a data perspective.
###############################
#####Mean Value Imputation#####
###############################
#Creating a dataset that has missing values.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
mean(missing.data$x2, na.rm = TRUE) #Mean of x2 prior to imputation.
sd(missing.data$x2, na.rm = TRUE) #Standard deviation of x2 prior to imputation.
cor(missing.data, use = "complete.obs") #Correlation prior to imputation.
#Mean value imputation method 1.
missing.data$x2[is.na(missing.data$x2)] = mean(missing.data$x2, na.rm=TRUE)
missing.data
#Mean value imputation method 2.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
missing.data = transform(missing.data, x2 = ifelse(is.na(x2),
mean(x2, na.rm=TRUE),
x2))
missing.data
#Mean value imputation method 3.
library(caret)
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
pre = caret::preProcess(missing.data, method = "medianImpute")
#Mean value imputation method 3.
install.packages('caret')
library(caret)
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
pre = caret::preProcess(missing.data, method = "medianImpute")
missing.data = predict(pre, missing.data)
missing.data
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
pre = caret::preProcess(missing.data, method = "medianImpute")
missing.data = predict(pre, missing.data)
missing.data
### Why Caret?
## 1. Maintain the structure of train - predict as other machine learning procedure.
##    This is particularly important when impute for future observation
## 2. Can be collected with other preprocesses, as below:
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
pre = preProcess(missing.data, method = c("scale", "medianImpute"))
missing.data = predict(pre, missing.data) #use predict function to predict missing data according to pre
missing.data
### Why Caret?
## 1. Maintain the structure of train - predict as other machine learning procedure.
##    This is particularly important when impute for future observation
## 2. Can be collected with other preprocesses, as below:
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
pre = preProcess(missing.data, method = c("scale", "medianImpute"))
missing.data = predict(pre, missing.data) #use predict function to predict missing data according to pre
missing.data
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
pre = preProcess(missing.data, method = c("center","scale", "medianImpute"))
missing.data = predict(pre, missing.data)
missing.data
#Mean value imputation method 4.
library(Hmisc) #Load the Harrell miscellaneous library.
#Mean value imputation method 4.
install.packages('Hmisc')
library(Hmisc) #Load the Harrell miscellaneous library.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
imputed.x2 = impute(missing.data$x2, mean) #Specifically calling the x2 variable.
imputed.x2
summary(imputed.x2) #Summary information for the imputed variable.
is.imputed(imputed.x2) #Boolean vector indicating imputed values.
missing.data$x2 = imputed.x2 #Replacing the old vector.
mean(missing.data$x2) #Mean of x2 after imputation.
sd(missing.data$x2) #Standard deviation of x2 after imputation.
cor(missing.data, use = "complete.obs") #Correlation afterto imputation.
plot(missing.data) #What are some potential problems with mean value imputation?
mean(missing.data$x2) #Mean of x2 after imputation.
sd(missing.data$x2) #Standard deviation of x2 after imputation.
cor(missing.data, use = "complete.obs") #Correlation afterto imputation.
plot(missing.data) #What are some potential problems with mean value imputation?
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
imputed.x2 = impute(missing.data$x2, mean) #Specifically calling the x2 variable.
imputed.x2
summary(imputed.x2) #Summary information for the imputed variable.
is.imputed(imputed.x2) #Boolean vector indicating imputed values.
missing.data$x2 = imputed.x2 #Replacing the old vector.
mean(missing.data$x2) #Mean of x2 after imputation.
sd(missing.data$x2) #Standard deviation of x2 after imputation.
cor(missing.data, use = "complete.obs") #Correlation afterto imputation.
plot(missing.data) #What are some potential problems with mean value imputation?
data = read.csv('./data/vrbo.csv', header=TRUE)
setwd('vrbo_shiny')
data = read.csv('./data/vrbo.csv', header=TRUE)
smoking <- read.csv(".data/US.csv", header = TRUE)
smoking <- read.csv("./data/US.csv", header = TRUE)
states <- geojson_read("gz_2010_us_040_00_500k.json",what = "sp")
library(sp)
library(leaflet)
library(geojsonio)
install.packages('geojsonio')
library(geojsonio)
states <- geojson_read("gz_2010_us_040_00_500k.json",what = "sp")
states <- geojson_read("gz_2010_us_040_00_500k.json",what = "sp")
library(sp)
states
states <- geojson_read("gz_2010_us_040_00_500k.json",what = "sp")
states
# Merge data
# require(sp)! For spatial dataframe!
smoking.df <- merge(states, smoking, by.x = "NAME", by.y = "state")
states <- geojson_read(".data/gz_2010_us_040_00_500k.json",what = "sp")
states <- geojson_read("./data/gz_2010_us_040_00_500k.json",what = "sp")
states
# Merge data
# require(sp)! For spatial dataframe!
smoking.df <- merge(states, smoking, by.x = "NAME", by.y = "state")
smoking.df
data %>%
select  (price) %>%
group_by(., price) %>%
summarise(count = length(price)) %>%
arrange (., desc(count))
data %>%
select(state, city) %>%
group_by(., state, city) %>%
summarise(count = length(city)) %>%
arrange (., desc(count))
library(dplyr)
data %>%
select(state, city) %>%
group_by(., state, city) %>%
summarise(count = length(city)) %>%
arrange (., desc(count))
data %>%
select(state, city) %>%
group_by(., state, city) %>%
summarise(count = length(city)) %>%
require(datasets)
data %>%
select(state, city) %>%
group_by(., state, city) %>%
summarise(count = length(city))
stateData = data %>%
select(state, city) %>%
group_by(., state, city) %>%
summarise(count = length(city))
stateFull = read.csv("./data/states_abb.csv", header = TRUE)
View(smoking)
stateData.df <- merge(stateData, stateFull, by.x = "state", by.y = "abb")
stateData.df <- merge(stateData, stateFull, by.x = "state", by.y = "Abb")
stateData
View(stateData.df)
stateData.df1 <- merge(states, stateData.df, by.x = "NAME", by.y = "Name")
stateData = data %>%
select(state) %>%
group_by(., state) %>%
summarise(count = length(state))
stateFull = read.csv("./data/states_abb.csv", header = TRUE)
stateData.df <- merge(stateData, stateFull, by.x = "state", by.y = "Abb")
stateData.df1 <- merge(states, stateData.df, by.x = "NAME", by.y = "Name")
output$map <- renderLeaflet({
leaflet(stateData.df1) %>%
addProviderTiles(providers$Stamen.TonerLite) %>%
setView(lng = -98.583, lat = 39.833, zoom = 4) %>%
addPolygons(data = smoking.df ,fillColor = ~pal2(selectedYear()),
popup = paste0("<strong>State: </strong>",
smoking.df$NAME),
color = "#BDBDC3",
fillOpacity = 0.8,
weight = 1)
})
shiny::runApp()
install.packages("geojsonio")
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
stateData = data %>%
select(state) %>%
group_by(., state) %>%
summarise(count = length(state))
abbtoFull = read.csv("./data/states_abb.csv", header = TRUE)
stateFull.df <- merge(stateData, abbtoFull, by.x = "state", by.y = "Abb")
states <- geojson_read("./data/gz_2010_us_040_00_500k.json",what = "sp")
stateData.df <- merge(states, stateFull.df, by.x = "NAME", by.y = "Name")
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
getwd()
runApp()
runApp()
ggplot(dataN, aes(x = state, y = price)) +
geom_point(aes(color = as.character(input$selected))) +
stat_smooth(method = "lm",
col = "#C42126",
se = FALSE,
size = 1)
ggplot(dataN, aes(x = state, y = price)) +
geom_point(aes(color = state)) +
stat_smooth(method = "lm",
col = "#C42126",
se = FALSE,
size = 1)
data = read.csv('./data/vrbo.csv', header=TRUE)
data
data = read.csv('./data/vrbo.csv', header=TRUE)
dataN = data
ggplot(dataN, aes(x = state, y = price)) +
geom_point(aes(color = state)) +
stat_smooth(method = "lm",
col = "#C42126",
se = FALSE,
size = 1)
ggplot(data, aes(x = state, y = price)) +
geom_point(aes(color = state)) +
stat_smooth(method = "lm",
col = "#C42126",
se = FALSE,
size = 1)
runApp()
ggplot(data %>%
select(., state, price) %>%
group_by(., state) %>%
arrange (., desc(state)) %>%
head(12), aes(x = state, y = price)) +
geom_point(aes(color = state)) +
stat_smooth(method = "lm",
col = "#C42126",
se = FALSE,
size = 1)
data %>%
select(., state, price) %>%
group_by(., state) %>%
arrange (., desc(state))
runApp()
ggplot(data %>%
select(., state, price) %>%
group_by(., state) %>%
distinct() %>%
arrange (., desc(state)) %>%
head(12), aes(x = state, y = price)) +
geom_point(aes(color = state)) +
stat_smooth(method = "lm",
col = "#C42126",
se = FALSE,
size = 1)
data %>%
select(., state, price) %>%
group_by(., state) %>%
distinct() %>%
arrange (., desc(state)) %>%
head(12)
data %>%
select(., state, price) %>%
distinct() %>%
arrange (., desc(state)) %>%
head(12)
data %>%
select(., state, price) %>%
distinct() %>%
arrange (., desc(state)) %>%
head(12)
runApp()
runApp()
ggplot(data %>%
select(., state, price) %>%
group_by(., state) %>%
distinct() %>%
arrange (., desc(state)) %>%
head(100), aes(x = state, y = price)) +
geom_point(aes(color = state)) +
stat_smooth(method = "lm",
col = "#C42126",
se = FALSE,
size = 1)
ggplot(data %>%
select(., state, price) %>%
group_by(., state) %>%
distinct() %>%
arrange (., desc(state)) %>%
head(100), aes(x = state, y = price)) +
geom_point(aes(color = state))
ggplot(data %>%
select(., state, price) %>%
group_by(., state) %>%
distinct() %>%
arrange (., desc(state)) %>%
head(500), aes(x = state, y = price)) +
geom_point(aes(color = state))
ggplot(data %>%
select(., state, price) %>%
group_by(., state) %>%
distinct() %>%
arrange (., desc(state)) %>%
head(500), aes(x = state, y = price)) +
geom_point(aes(color = state))+ylim(0, 150)
ggplot(data %>%
select(., house, price) %>%
group_by(., state) %>%
distinct() %>%
arrange (., desc(house)) %>%
head(500), aes(x = house, y = price)) +
geom_point(aes(color = state))+ylim(0, 150)
ggplot(data %>%
select(., house, price) %>%
group_by(house) %>%
distinct() %>%
arrange (., desc(house)) %>%
head(500), aes(x = house, y = price)) +
geom_point(aes(color = state))+ylim(0, 150)
ggplot(data %>%
select(., house, price) %>%
group_by(house) %>%
distinct() %>%
arrange (., desc(house)) %>%
head(500), aes(x = house, y = price)) +
geom_point(aes(color = house))+ylim(0, 150)
ggplot(dataN %>%
select(., house, price) %>%
group_by(house) %>%
distinct() %>%
arrange (., desc(house)) %>%
head(500), aes(x = house, y = price)) +
geom_point(aes(color = house))+ylim(0, 150)
ggplot(dataN %>%
select(., house, price) %>%
group_by(house) %>%
distinct() %>%
arrange (., desc(house)) %>%
head(300), aes(x = house, y = price)) +
geom_point(aes(color = house))+ylim(0, 150)
ggplot(dataN %>%
select(., house, price) %>%
group_by(house) %>%
distinct() %>%
arrange (., desc(house)) %>%
head(300), aes(x = house, y = price)) +
geom_point(aes(color = house))+ylim(0, 250)
runApp()
runApp()
runApp()
runApp()
runApp()
nrow(stateData.df
nrow(stateData.df)
nrow(stateData.df)
nrow(stateData)
nrow(stateData)
nrow(data)
runApp()
runApp()
runApp()
runApp()
runApp()
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)
# Your code here
help(cats)
# Your code here
library(MASS)
data(cats)
help(cats)
cats = data(cats)
head(cats)
data(cats)
# Your code here
library(MASS)
data(cats)
head(cats)
# Your code here
library(ggplot2)
ggplot(data = cats, aes(x = Hwt, y = Bwt)) + geom_point()
ggplot(data = cats, aes(x = Hwt, y = Bwt)) + geom_point() + geom_smooth()
#Basic numerical EDA for cars dataset.
summary(cars) #Five number summaries.
sapply(cars, sd) #Standard deviations.
cor(cars) #Correlations.
##############################################
#####Manual example with the cars dataset#####
##############################################
help(cars)
cars #Investigating the cars dataset.
cor(cats)
# Your code here
summary(cats)
sapply(cats, sd)
#Basic graphical EDA for cars dataset.
hist(cars$speed, xlab = "Speed in MPH", main = "Histogram of Speed")
hist(cars$dist, xlab = "Distance in Feet", main = "Histogram of Distance")
beta1 = sum((cars$Hwt - mean(cars$Hwt)) * (cars$Bwt - mean(cars$Bwt))) /
sum((cars$Hwt - mean(cars$Hwt))^2)
beta0 = mean(cars$Bwt) - beta1*mean(cars$Hwt)
#Adding the least squares regression line to the plot.
abline(beta0, beta1, lty = 2)
#Manual calculation of simple linear regression coefficients.
beta1 = sum((cars$speed - mean(cars$speed)) * (cars$dist - mean(cars$dist))) /
sum((cars$speed - mean(cars$speed))^2)
beta0 = mean(cars$dist) - beta1*mean(cars$speed)
#Adding the least squares regression line to the plot.
abline(beta0, beta1, lty = 2)
plot(cars, xlab = "Speed in MPH", ylab = "Distance in Feet",
main = "Scatterplot of Cars Dataset")
#Manual calculation of simple linear regression coefficients.
beta1 = sum((cars$speed - mean(cars$speed)) * (cars$dist - mean(cars$dist))) /
sum((cars$speed - mean(cars$speed))^2)
beta0 = mean(cars$dist) - beta1*mean(cars$speed)
#Adding the least squares regression line to the plot.
abline(beta0, beta1, lty = 2)
plot(cats)
ggplot(data = cats, aes(x = Hwt, y = Bwt)) + geom_point() + geom_smooth()
ggplot(data = cats, aes(x = Hwt, y = Bwt)) + geom_point() + geom_smooth()
ggplot(data = cats, aes(x = Hwt, y = Bwt)) + geom_point() + geom_smooth() + abline(beta0, beta1, lty = 2)
#################################################
#####Automatic example with the cars dataset#####
#################################################
model = lm(dist ~ speed, data = cars) #Use the linear model function lm() to
####################################################
#####Checking assumptions with the cars dataset#####
####################################################
#Linearity
plot(cars, xlab = "Speed in MPH", ylab = "Distance in Feet",
main = "Scatterplot of Cars Dataset")
abline(model, lty = 2)
#################################################
#####Automatic example with the cars dataset#####
#################################################
model = lm(dist ~ speed, data = cars) #Use the linear model function lm() to
model = lm(Hwt ~ Bwt, data = cats)
summary(model)
confint(model)
cor(cats$Hwt, cats$Bwt)
summary(model)
abline(model, lty = 2)
####################################################
#####Checking assumptions with the cars dataset#####
####################################################
#Linearity
plot(cars, xlab = "Speed in MPH", ylab = "Distance in Feet",
main = "Scatterplot of Cars Dataset")
abline(model, lty = 2)
ggplot(data = cats, aes(x = Hwt, y = Bwt)) + geom_point() + geom_smooth(method = lm)
ggplot(data = cats, aes(x = Hwt, y = Bwt)) + geom_point() + geom_smooth(method = lm)
# Your code here
ggplot(data = cats, aes(x = Hwt, y = Bwt)) + geom_point() + geom_smooth(method = lm)
ggplot(data = cats, aes(x = Hwt, y = Bwt)) +
geom_point() +
stat_smooth(method = lm, se = FALSE) +
geom_segment(aes(xend = Hwt, yend = .fitted), color = "red", size = 0.3)
ggplot(data = cats, aes(x = Hwt, y = Bwt)) +
geom_point() +
stat_smooth(method = lm, se = FALSE) +
geom_segment(aes(xend = Hwt, yend = Bwt), color = "red", size = 0.3)
bc = boxCox(model) #Automatically plots a 95% confidence interval for the lambda
####################################
#####The Box-Cox Transformation##### #use this to predict the y better
####################################
library(car)
bc = boxCox(model) #Automatically plots a 95% confidence interval for the lambda
# Your code here
bc = boxCox(model)
# Your code here
bc = boxCox(model)
lambda = bc$x[which(bc$y == max(bc$y))] #Extracting the best lambda value.
dist.bc = (cats$Bwt^lambda - 1)/lambda #Applying the Box-Cox transformation.
hist(dist.bc)
model.bc = lm(Bwt.bc ~ cats$Hwt) #Creating a new regression based on the
summary(model.bc) #Assessing the output of the new model.
hist(Bwt.bc)
model.bc = lm(Bwt.bc ~ cats$Hwt) #Creating a new regression based on the
Bwt.bc = (cats$Bwt^lambda - 1)/lambda #Applying the Box-Cox transformation.
hist(Bwt.bc)
model.bc = lm(Bwt.bc ~ cats$Hwt) #Creating a new regression based on the
summary(model.bc) #Assessing the output of the new model.
plot(model.bc) #Assessing the assumptions of the new model.
#####################################
#####Predicting New Observations#####
#####################################
model$fitted.values #Returns the fitted values.
